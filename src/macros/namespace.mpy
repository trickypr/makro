from tokens import TokenCase, Tokens
from macros.utils import camel_to_snake
from macros.types import MacroParser, MacroTranslator
from macros.pyx import create_class, create_func, program, string_as_token
from typing import List
import tokenize
from argparse import Namespace
macro import enum


class Visit():
    def visit(self, visitor: any) -> List[tokenize.TokenInfo]:
        return visitor.__getattribute__(camel_to_snake(
            self.__class__.__name__))(self)


enum NamespaceAST(Visit):
    Statement(identifier: tokenize.TokenInfo)
    Body(contents: str, stmts: List[NamespaceAST])
    Namespace(identifier: tokenize.TokenInfo, body: NamespaceAST)


class Parser(MacroParser):
    def body(self, tokens: Tokens):
        # <body> ::= <statement> {'\n' <statement>}
        statements = []
        lines = []

        current_line = -1
        indent = 1
        tab = ' ' * 4

        while not tokens.is_at_end():
            if current_line != tokens.previous().start[0]:
                current_line = tokens.previous().start[0]
                lines.append(tokens.previous().line)

            if tokens.match(TokenCase().type(tokenize.NAME).string('export')):
                tokens.advance()
                identifier = tokens.consume(TokenCase().type(
                    tokenize.NAME), "Expected identifier")
                statements.append(NamespaceAST.Statement(identifier))

                # Remove export from the last line
                last_line = lines.pop()
                tab = last_line.split('export')[0]
                lines.append(tab + last_line.replace('export', '', 1).strip())

            if tokens.match(TokenCase().type(tokenize.INDENT)):
                indent += 1
            elif tokens.match(TokenCase().type(tokenize.DEDENT)):
                indent -= 1
            else:
                tokens.advance()

            if indent <= 0:
                # HACK: The last option here will be an extra line that we don't
                # want. Instead of fixing the bug, lets just remove the extra
                # line
                lines.pop()
                break

        if tokens.is_at_end():
            tokens.error("Expected dedent, found end of file")

        return NamespaceAST.Body('\n'.join([line.replace(tab, '', 1) for line in lines]), statements)

    def parse(self, tokens: Tokens) -> any:
        identifier = tokens.consume(TokenCase().type(
            tokenize.NAME), 'Expected namespace identifier')

        tokens.consume(TokenCase().type(
            tokenize.OP).string(':'), "Expected ':'")
        tokens.consume(TokenCase().type(tokenize.NEWLINE), "Expected newline")
        tokens.consume(TokenCase().type(tokenize.INDENT), "Expected indent")

        return NamespaceAST.Namespace(identifier, self.body(tokens))


class Translator(MacroTranslator):
    def statement(self, ast: NamespaceAST.Body) -> List[tokenize.TokenInfo]:
        return [string_as_token(f'{ast.identifier.string}: {ast.identifier.string}')]

    def body(self, ast: NamespaceAST.Body):
        return_def = 'return { ' + \
            ", ".join([export.visit(self) for export in ast.stmts]) + ' }'

        return program(
            *ast.contents.split('\n'),
            string_as_token(return_def))

    def namespace(self, ast: NamespaceAST.Namespace) -> List[tokenize.TokenInfo]:
        namespace_def_function = f'__{ast.identifier.string}_namespace_creator'

        namespace_exports = [
            f'self.{export.identifier.string} = {export.identifier.string}' for export in ast.body.stmts]
        namespace_class = create_class(namespace_def_function, create_func('__init__', 'self', program(
            string_as_token(ast.body.contents), *map(lambda x: string_as_token(x), namespace_exports))))

        return program(
            string_as_token(f'# Start of namespace {ast.identifier.string}'),
            *namespace_class,

            string_as_token(
                f'{ast.identifier.string} = {namespace_def_function}()'),
            string_as_token(f'del {namespace_def_function}'),
            string_as_token(''),
            string_as_token(''),
            string_as_token(f'# End of namespace {ast.identifier.string}'))

    def translate(self, ast: NamespaceAST.Namespace) -> List[tokenize.TokenInfo]:
        return ast.visit(self)
